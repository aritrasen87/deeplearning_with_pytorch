{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this tutorial ,  we will create a Deep Learning model for building a handwritten digit classifier. We will make use of the MNIST dataset included in the torchvision package.\n",
    " \n",
    " Mandatory first step is to do the basic data pre-processing steps , using the a utility called transforms which comes from \n",
    " torchvision package we will do two below mentioned basic data preprocessing operations (this will be explained more detail in case of CNN).\n",
    " \n",
    "- Transform the raw dataset into tensors.\n",
    "- Normalize the dataset.\n",
    "\n",
    "We will also import the dataset from torch vision package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity we are only using the training dataset in this tutorial\n",
    "\n",
    "Test and Validation dataset have been used in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training dataset\n",
    "train_data = MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of train dataset\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data loader preparation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creates iterables for all the batches and we will use this inside the traning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of the input/target data\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential:\n",
    "\n",
    "Below we have defined the Deep Neural Network archietecture with the help of nn.Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.3),\n",
    "                      nn.Linear(512, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Support:\n",
    "First check that your GPU is working in Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Creating a device object \n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the model to avialable 'device'\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Traning the Model:<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  1.0559210053210457\n",
      "Epoch: 2 Training Loss:  0.4704279328820606\n",
      "Epoch: 3 Training Loss:  0.3786520552448928\n",
      "Epoch: 4 Training Loss:  0.3260291467048228\n",
      "Epoch: 5 Training Loss:  0.28475500740421317\n",
      "Epoch: 6 Training Loss:  0.25306461652430395\n",
      "Epoch: 7 Training Loss:  0.23030062804309032\n",
      "Epoch: 8 Training Loss:  0.21214023194275797\n",
      "Epoch: 9 Training Loss:  0.19736038566101344\n",
      "Epoch: 10 Training Loss:  0.1836888543376699\n",
      "Epoch: 11 Training Loss:  0.17243863903917372\n",
      "Epoch: 12 Training Loss:  0.1637192848386864\n",
      "Epoch: 13 Training Loss:  0.15155088650450732\n",
      "Epoch: 14 Training Loss:  0.14521185631630942\n",
      "Epoch: 15 Training Loss:  0.1373947342322208\n",
      "Wall time: 5min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, 16): ## run the model for 15 epochs\n",
    "    train_loss = []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Move input and label tensors to the avialable device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #Reshaping the input data before sending into the model\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load The Model:\n",
    "As now the model has been trained , we will save the model and load again for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing our model: \n",
      "\n",
      " Sequential(\n",
      "  (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3)\n",
      "  (3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.5)\n",
      "  (6): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"printing our model: \\n\\n\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the weights and biases of the model \n",
    "\n",
    "The parameters for PyTorch models are stored in a model's state_dict. state_dict containts the weights & biases of each of the layer , which can be accesed by <b>state_dict().keys()</b>. \n",
    "\n",
    "Below we can see that , every layer's weight and biases have been printed out -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models layer keys: \n",
      "\n",
      " odict_keys(['0.weight', '0.bias', '3.weight', '3.bias', '6.weight', '6.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Models layer keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Weights and Bias Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight : tensor([[ 0.0233,  0.0113,  0.0321,  ...,  0.0232,  0.0103,  0.0278],\n",
      "        [-0.0340,  0.0217, -0.0018,  ...,  0.0128,  0.0121,  0.0353],\n",
      "        [-0.0092,  0.0054,  0.0285,  ...,  0.0012,  0.0117, -0.0085],\n",
      "        ...,\n",
      "        [-0.0331,  0.0108,  0.0326,  ..., -0.0342, -0.0133,  0.0317],\n",
      "        [ 0.0123, -0.0274,  0.0345,  ..., -0.0264,  0.0374, -0.0304],\n",
      "        [ 0.0237, -0.0239, -0.0043,  ...,  0.0268,  0.0203, -0.0229]])\n"
     ]
    }
   ],
   "source": [
    "for params, values in model.state_dict().items(): \n",
    "    print(params, \":\", values)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model's statedict can be saved using the torch.save which also accepts the models name as parameter as - <b>model.pth</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved model can also be loaded using the <b>torch.load()</b> using the saved model's path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '3.weight', '3.bias', '6.weight', '6.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('model.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the state dict in to the new model, you do <b>model.load_state_dict(state_dict)</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Please Note:</b> Loading the state dict will work only if the new model architecture is exactly the same as the saved's model's architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
