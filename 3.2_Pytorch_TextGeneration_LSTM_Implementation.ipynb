{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coding example - we will train a model to generate new text using LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Class:\n",
    "As we saw in the tutorial that before passing the words to the Embedding layer, we need a create a word to index mappin. \n",
    "This word2Index mapping we will call a dictionary. Dictionary class does below mentioned tasks -\n",
    "\n",
    "- At first it checks if the passed word is already present in the dictionary or not.\n",
    "- If it's a new word the class adds the word to the dictionary , assigns an index to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Class:\n",
    "\n",
    "Corpus class with the help of Dictionary class doing the below operations - \n",
    "\n",
    "- Creates an object of the Dictionary class which is to be used in future operations. \n",
    "- In the get_data method below operations are going on - \n",
    "    - Opens the file in read mode.\n",
    "    - Reads the file line by line , splits each of the words in lines.\n",
    "    - Adds a end of sentence tokoen at the end of each line.\n",
    "    - Maintains a variable tokens to keep track of the total number of words.\n",
    "    - Adds the words in the dictionary. \n",
    "    - Once all the words are added in the dictonary creates a long tensor named 'ids'\n",
    "    - In the 'ids' all the index from the dictionary is stored using word2idx.\n",
    "    - Makes sure that all batches are of same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        #Create a 1-D tensor which contains index of all the words in the file with the help of word2idx\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        # no of required batches            \n",
    "        num_batches = ids.shape[0] // batch_size     \n",
    "        #Remove the remainder from the last batch , so that always batch size is constant\n",
    "        ids = ids[:num_batches*batch_size]\n",
    "        # return (batch_size,num_batches)\n",
    "        ids = ids.view(batch_size, -1)\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128    # Embedding layer size , input to the LSTM\n",
    "hidden_size = 1024  # Hidden size of LSTM units\n",
    "num_layers = 1      # no LSTMs stacked\n",
    "num_epochs = 10     # total no of epochs\n",
    "batch_size = 20     # batch size\n",
    "seq_length = 100     # sequence length\n",
    "learning_rate = 0.002 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the get data method with path of the file and batch size\n",
    "Data Source - https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = corpus.get_data('data.txt', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 18970])\n"
     ]
    }
   ],
   "source": [
    "# ids tensors contain all the index of each words\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    2,  ...,  737,  181,  247],\n",
       "        [  42,   32,   79,  ..., 1132,   27,   27],\n",
       "        [ 467,   27,   27,  ...,   24,  130,  154],\n",
       "        ...,\n",
       "        [  42,   26,   48,  ...,   32,  392,   34],\n",
       "        [  35, 3039,   24,  ..., 3154, 1339, 1570],\n",
       "        [1088,   24,  315,  ...,  108, 1691,   27]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9468\n"
     ]
    }
   ],
   "source": [
    "# What is the vocabulary size ?\n",
    "vocab_size = len(corpus.dictionary)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our sequence length is greater than one , so multiple words would be there in a single batch. So we will need 10 batches to pass through the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "num_batches = ids.shape[1] // seq_length\n",
    "print(num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model archietecture with LSTM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) # maps words to feature vectors\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) # LSTM layer\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size) # Fully connected layer\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        # Perform Word Embedding \n",
    "        x = self.embed(x)\n",
    "\n",
    "        out, (h, c) = self.lstm(x, h) # (input , hidden state)\n",
    "        \n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        \n",
    "        # Decode hidden states of all time steps\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(vocab_size, embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch LSTM document - https://pytorch.org/docs/stable/nn.html#lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to Detach the Hidden and Cell states from previous history\n",
    "def detach(states):\n",
    "    return [state.detach() for state in states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.1580\n",
      "Epoch [1/10], Loss: 6.0836\n",
      "Epoch [2/10], Loss: 5.4496\n",
      "Epoch [2/10], Loss: 5.3647\n",
      "Epoch [3/10], Loss: 4.7479\n",
      "Epoch [3/10], Loss: 4.8044\n",
      "Epoch [4/10], Loss: 4.1976\n",
      "Epoch [4/10], Loss: 4.2439\n",
      "Epoch [5/10], Loss: 3.6642\n",
      "Epoch [5/10], Loss: 3.7502\n",
      "Epoch [6/10], Loss: 3.1097\n",
      "Epoch [6/10], Loss: 3.2812\n",
      "Epoch [7/10], Loss: 2.6329\n",
      "Epoch [7/10], Loss: 2.8812\n",
      "Epoch [8/10], Loss: 2.2548\n",
      "Epoch [8/10], Loss: 2.4805\n",
      "Epoch [9/10], Loss: 1.9408\n",
      "Epoch [9/10], Loss: 2.1896\n",
      "Epoch [10/10], Loss: 1.7228\n",
      "Epoch [10/10], Loss: 1.9036\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # initial hidden and cell states\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size),\n",
    "              torch.zeros(num_layers, batch_size, hidden_size))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        \n",
    "        #move with seq length from the the starting index and move till - (ids.size(1) - seq_length)\n",
    "        \n",
    "        # prepare mini-batch inputs and targets\n",
    "        inputs = ids[:, i:i+seq_length] # fetch words for one seq length  \n",
    "        targets = ids[:, (i+1):(i+1)+seq_length] # shifted by one word from inputs\n",
    "        \n",
    "        states = detach(states)\n",
    "\n",
    "        outputs,states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "         \n",
    "        #The gradients are clipped in the range [-clip_value, clip_value]. This is to prevent the exploding gradient problem\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "              \n",
    "        step = (i+1) // seq_length\n",
    "        if step % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geneate new Text using the training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled [100/500] words and save to results.txt\n",
      "Sampled [200/500] words and save to results.txt\n",
      "Sampled [300/500] words and save to results.txt\n",
      "Sampled [400/500] words and save to results.txt\n",
      "Sampled [500/500] words and save to results.txt\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    with open('results.txt', 'w') as f:\n",
    "        #intial hidden ane cell states\n",
    "        state = (torch.zeros(num_layers, 1, hidden_size),\n",
    "                 torch.zeros(num_layers, 1, hidden_size))\n",
    "        \n",
    "        # Select one word id randomly and convert it to shape (1,1)\n",
    "        input = torch.randint(0,vocab_size, (1,)).long().unsqueeze(1) \n",
    "                            # (min , max , shape) , convert to long tensor and make it a shape of 1,1 \n",
    "\n",
    "        for i in range(500):\n",
    "            output, _ = model(input, state)\n",
    "\n",
    "            \n",
    "            # Sample a word id from the exponential of the output \n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "            #print(word_id)\n",
    "\n",
    "            \n",
    "            # Replace the input with sampled word id for the next time step\n",
    "            input.fill_(word_id)\n",
    "\n",
    "            # Write the results to file\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word + ' '\n",
    "            f.write(word)\n",
    "\n",
    "            \n",
    "            if (i+1) % 100 == 0:\n",
    "                print('Sampled [{}/{}] words and save to {}'.format(i+1, 500, 'results.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why use multinomial distribution?\n",
    "\n",
    "Sticking to the most probable words would restrict the model to always use the most commonly used words, just to add some randomness we use this approach.\n",
    "\n",
    "I tried top k approach, howver top k approach was kind of generating same words for so used the multinomial distribution approach \n",
    "\n",
    "- Multinomial pytorch documentation - https://pytorch.org/docs/stable/torch.html?highlight=multinomial#torch.multinomial\n",
    "- What is Multinomial distribution ? https://www.youtube.com/watch?v=syVW7DgvUaY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Inspiration : https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
