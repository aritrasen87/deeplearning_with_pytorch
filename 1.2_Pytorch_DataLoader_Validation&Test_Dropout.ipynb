{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this tutorial ,  we will create a Deep Learning model for building a handwritten digit classifier. We will make use of the MNIST dataset included in the torchvision package.\n",
    " \n",
    " Mandatory first step is to do the basic data pre-processing steps , using the a utility called transforms which comes from \n",
    " torchvision package we will do two below mentioned basic data preprocessing operations (this will be explained more detail in case of CNN).\n",
    " \n",
    "- Transform the raw dataset into tensors.\n",
    "- Normalize the dataset.\n",
    "\n",
    "We will also import the dataset from torch vision package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # The values 0.1307 and 0.3081 used for the Normalize() transformation are the global mean and standard deviation of the MNIST dataset.\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Test Dataset:\n",
    "When creating a machine learning model, the ultimate goal is for it to be accurate on new/unseen data, not just the data you are using to build it. The Idea of Validation set is to see if there is any overfitting going on or not. \n",
    "\n",
    "As this blog post by fast.ai says (https://www.fast.ai/2017/11/13/validation-sets/) about the different datasets:\n",
    "\n",
    "- The training set is used to train a given model\n",
    "- The validation set is used to choose between models (for instance, does a random forest or a neural net work better for your problem? do you want a random forest with 40 trees or 50 trees?)\n",
    "- The test set tells you how you’ve done. If you’ve tried out a lot of different models, you may get one that does well on your validation set just by chance, and having a test set helps make sure that is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training and test datasets\n",
    "train_data = MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of train and test data\n",
    "len(train_data) , len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders:\n",
    "\n",
    "Few Terminology to understand first:\n",
    "\n",
    "When the no of tranining examples are very big , then we don't pass the dataset for traninig , we create batches first and then do one forward pass + one backward pass.\n",
    "\n",
    "- <b>epoch</b>: one forward pass + one backward pass of all traning example\n",
    "- <b>batch size</b>: number of traning examples in one epoch.\n",
    "- <b>Iterations</b>: if you have 1000 no of traning examples(or rows) and your batch size is 100 then you will need 10 iterations to complete one epoch.\n",
    "\n",
    "Pytorch's DataLoader is responsible for managing & creating batches. DataLoader makes it easier to iterate over batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is SubsetRandomSampler?\n",
    "\n",
    "- syntax - torch.utils.data.SubsetRandomSampler(indices)\n",
    "- Samples elements randomly from a given list of indices, without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Here we will use a subset of traning set for validation\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "ix = list(range(num_train))\n",
    "np.random.shuffle(ix)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = ix[split:], ix[:split]\n",
    "\n",
    "# create sampler objects using SubsetRandomSampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# data loaders preparation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler)\n",
    "valid_loader = DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creates iterables for all the batches and we will use this inside the traning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of the input/target data\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Dropout or Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 56)\n",
    "        self.fc5 = nn.Linear(56, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input tensor is flattened \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # without applying dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  1.7596560213714838 Valid Loss:  0.6579833876341581\n",
      "Epoch: 2 Training Loss:  0.43651929814368484 Valid Loss:  0.3462761305272579\n",
      "Epoch: 3 Training Loss:  0.2889881295229619 Valid Loss:  0.2543290414536993\n",
      "Epoch: 4 Training Loss:  0.2164723220669354 Valid Loss:  0.19744457413908095\n",
      "Epoch: 5 Training Loss:  0.1673476675253672 Valid Loss:  0.16588785587033877\n",
      "Epoch: 6 Training Loss:  0.13652527926509114 Valid Loss:  0.14572855938846865\n",
      "Epoch: 7 Training Loss:  0.1135441082015556 Valid Loss:  0.1232484115442882\n",
      "Epoch: 8 Training Loss:  0.09618703598389403 Valid Loss:  0.11893888727451364\n",
      "Epoch: 9 Training Loss:  0.08229181061615236 Valid Loss:  0.10581552037037908\n",
      "Epoch: 10 Training Loss:  0.07037087102168395 Valid Loss:  0.09994099684408866\n",
      "Epoch: 11 Training Loss:  0.06042850601297687 Valid Loss:  0.09403780558495782\n",
      "Epoch: 12 Training Loss:  0.0519398271649455 Valid Loss:  0.09183392282575369\n",
      "Epoch: 13 Training Loss:  0.04416612840614107 Valid Loss:  0.09807272924808785\n",
      "Epoch: 14 Training Loss:  0.03758015634327118 Valid Loss:  0.08905830568401144\n",
      "Epoch: 15 Training Loss:  0.032285431055667384 Valid Loss:  0.08626947776356246\n",
      "Epoch: 16 Training Loss:  0.027014516549146113 Valid Loss:  0.08702167127921712\n",
      "Epoch: 17 Training Loss:  0.022869515288524173 Valid Loss:  0.08851984815943675\n",
      "Epoch: 18 Training Loss:  0.01974819303171292 Valid Loss:  0.08516235913557466\n",
      "Epoch: 19 Training Loss:  0.016494073682467084 Valid Loss:  0.08478335111091534\n",
      "Epoch: 20 Training Loss:  0.013408612621969952 Valid Loss:  0.08746147480026896\n",
      "Epoch: 21 Training Loss:  0.010941006698764492 Valid Loss:  0.08804350007291456\n",
      "Epoch: 22 Training Loss:  0.00921479660823934 Valid Loss:  0.09048483841046012\n",
      "Epoch: 23 Training Loss:  0.007846790260691705 Valid Loss:  0.08977286126961796\n",
      "Epoch: 24 Training Loss:  0.0063853708185282205 Valid Loss:  0.09039550049274112\n",
      "Epoch: 25 Training Loss:  0.005556152621799508 Valid Loss:  0.0891483135314047\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 26): ## run the model for 25 epochs\n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ## evaluation part\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss.append(loss.item())\n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome:\n",
    "So if we look closely at the loss we can see that till epoch 10 , training loss and validation loss is more or less very close , howver after that you training loss is getting decreased but your test is not getting decreased at all , it's almost constant. Which mean there is <b>Over fitting</b> problem i.e. your model not getting generalized very well. This can be solved by adding drop outs to the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout:\n",
    "A simple but effective regularization technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. Dropout is again used to reduce the '<b>overfitting problem</b>'. Drop is more useful when we have deep network. We give a dropout probablity(to switch off the weights randomly) in the configuration. \n",
    "\n",
    "Dropout is generally used during the training phase only and we switch off dropout during test/validation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 56)\n",
    "        self.fc5 = nn.Linear(56, 10)\n",
    "        \n",
    "        #drop out with 0.3 probability\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input tensor is flattened \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # applied dropout layer\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = self.dropout(F.relu(self.fc4(x)))\n",
    "        \n",
    "        #no dropout at the output layer\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Traning the Model:<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  2.070287090912461 Valid Loss:  1.127383054792881\n",
      "Epoch: 2 Training Loss:  0.8999231449328363 Valid Loss:  0.46199527066200974\n",
      "Epoch: 3 Training Loss:  0.5342848896048963 Valid Loss:  0.3163908563864728\n",
      "Epoch: 4 Training Loss:  0.3998483529081568 Valid Loss:  0.24400846174297233\n",
      "Epoch: 5 Training Loss:  0.31707047269834826 Valid Loss:  0.2026040290482342\n",
      "Epoch: 6 Training Loss:  0.268898129540806 Valid Loss:  0.17118983695593973\n",
      "Epoch: 7 Training Loss:  0.23212495798555513 Valid Loss:  0.1494857892510481\n",
      "Epoch: 8 Training Loss:  0.20526526528410613 Valid Loss:  0.13463826690955707\n",
      "Epoch: 9 Training Loss:  0.18597802157552604 Valid Loss:  0.12460846348161188\n",
      "Epoch: 10 Training Loss:  0.1662489805467582 Valid Loss:  0.11330763933171208\n",
      "Epoch: 11 Training Loss:  0.1523863657513478 Valid Loss:  0.10701410834056636\n",
      "Epoch: 12 Training Loss:  0.14110717593575828 Valid Loss:  0.1003860851478142\n",
      "Epoch: 13 Training Loss:  0.12830972906861765 Valid Loss:  0.09508630341927832\n",
      "Epoch: 14 Training Loss:  0.12037181839793144 Valid Loss:  0.09425928233443605\n",
      "Epoch: 15 Training Loss:  0.10950063896695307 Valid Loss:  0.09069388477073517\n",
      "Epoch: 16 Training Loss:  0.10307603092999974 Valid Loss:  0.08618099341935401\n",
      "Epoch: 17 Training Loss:  0.09576740692718885 Valid Loss:  0.08363322188427749\n",
      "Epoch: 18 Training Loss:  0.08948019138721672 Valid Loss:  0.0835584420102047\n",
      "Epoch: 19 Training Loss:  0.08480429548386989 Valid Loss:  0.08122900391657216\n",
      "Epoch: 20 Training Loss:  0.08121758858469548 Valid Loss:  0.08343449076395094\n",
      "Epoch: 21 Training Loss:  0.07529573429880353 Valid Loss:  0.0800956018579503\n",
      "Epoch: 22 Training Loss:  0.06892609414110969 Valid Loss:  0.07822207294993859\n",
      "Epoch: 23 Training Loss:  0.06720452966934924 Valid Loss:  0.07757362427000772\n",
      "Epoch: 24 Training Loss:  0.06512606966801589 Valid Loss:  0.08146156963848625\n",
      "Epoch: 25 Training Loss:  0.059786614157201254 Valid Loss:  0.07513980538593994\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 26): ## run the model for 25 epochs\n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ## evaluation part\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss.append(loss.item())\n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Few Steps to note:\n",
    "\n",
    "- <b>torch.no_grad()</b>: impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop. We generally don't want backpropagation in validation and test phase.\n",
    "- <b>model.eval()</b>: This will switch off the dropouts for validation phase. \n",
    "- <b>model.train()</b>: Will bring the model again into traning phase by switching on the dropouts.\n",
    "\n",
    "If the loss of traning set and validation sets are very close that means there is less overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the network\n",
    "See the performence on the test dataset and also check the classwise accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.074737\n",
      "\n",
      "Test Accuracy of     0: 99% (971/980)\n",
      "Test Accuracy of     1: 99% (1126/1135)\n",
      "Test Accuracy of     2: 98% (1017/1032)\n",
      "Test Accuracy of     3: 98% (993/1010)\n",
      "Test Accuracy of     4: 97% (960/982)\n",
      "Test Accuracy of     5: 97% (869/892)\n",
      "Test Accuracy of     6: 98% (940/958)\n",
      "Test Accuracy of     7: 96% (996/1028)\n",
      "Test Accuracy of     8: 97% (949/974)\n",
      "Test Accuracy of     9: 97% (983/1009)\n",
      "\n",
      "Test Accuracy (Overall): 98% (9804/10000)\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    #test_loss.append(loss.item())\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding L2 Regularization to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "```\n",
    "\n",
    "You can specify the weight_decay lamda parameter values while defining the model optimizer.\n",
    "\n",
    "Higher the value of weight_decay higher the shrinkage in the model weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "1758999f029eddf959a1186ac4cb199870fecf5394707e39dfa8e19edac1c51d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
