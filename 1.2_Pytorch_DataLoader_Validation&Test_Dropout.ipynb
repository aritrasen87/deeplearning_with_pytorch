{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section one sample one how to train a neaural network from scratch. In this notebook we'll use nn.Module and nn.Parameter along with autograd, optim utility packages provided within PyTorch. We subclass nn.Module (which itself is a class and able to keep track of state). nn.Module has a number of attributes and methods (such as .parameters() and .zero_grad()) which we will be using to perform different operations required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this tutorial ,  we will create a Deep Learning model for building a handwritten digit classifier. We will make use of the MNIST dataset included in the torchvision package.\n",
    " \n",
    " Mandatory first step is to do the basic data pre-processing steps , using the a utility called transforms which comes from \n",
    " torchvision package we will do two below mentioned basic data preprocessing operations (this will be explained more detail in case of CNN).\n",
    " \n",
    "- Transform the raw dataset into tensors.\n",
    "- Normalize the dataset.\n",
    "\n",
    "We will also import the dataset from torch vision package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training and test datasets\n",
    "train_data = MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of train and test data\n",
    "len(train_data) , len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "ix = list(range(num_train))\n",
    "np.random.shuffle(ix)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = ix[split:], ix[:split]\n",
    "\n",
    "# create sampler objects using SubsetRandomSampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# data loaders preparation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of the input/target data\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that input data is not flattened to pass through the linear layers , so we will need to reshape the batch in a format of (batch size , no of features(1*28*28 = 784)).Shape of target data is an expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "We create a new class (which inherits the properties from the base class from nn package called Module) to define the archietecture of the Neaural Network. \n",
    "\n",
    "- Layer defination should be inside the constructor of the class.\n",
    "- Forward propagation step should be included inside forward method.\n",
    "\n",
    "Activations(Relu,Sigmoid,Tanh etc) and loss functions(cross entropy,softmax etc) comes from torch.nn.functional. This module contains all the functions in the torch.nn module.\n",
    "\n",
    "Syntax of nn.Linear() is (input size, output size)\n",
    "\n",
    "This NN architecture below represents the 784 nodes (28*28 pixels) in the input layer, 256 in the hidden layer, and 10 in the output layer(0-9 numbers). Inside the forward function, we will use the relu activation function in the hidden layer which present under torch.nn.functional module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input tensor is flattened \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the loss and optimizer functions\n",
    "\n",
    "- <u><b>Loss Function</b></u>:\n",
    "\n",
    "Here we have used the CrossEntropyLoss() function \n",
    "Generally loss assigned to `criterion`. for MNIST classification , we generally use softmax function to predict class probabilities. With a softmax output, you want to use cross-entropy as the loss.\n",
    "\n",
    "Few things to keep in mind while using the CrossEntropyLoss() in Pytorch \n",
    "\n",
    "    - CrossEntropyLoss criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "    - The input is expected to contain scores for each class.\n",
    "\n",
    "This means we need to pass in the raw output of our network into the loss, not the output of the softmax function.That's why there is no activation after self.fc2(x).\n",
    "\n",
    "- <u><b>Optimizer</b></u>:\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, torch.optim. We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.Below are the few availavle optimizer in pytorch -\n",
    "\n",
    "    - optim.Adam\n",
    "    - optim.RMSprop\n",
    "    - optim.SGD\n",
    "    - optim.Adagrad\n",
    " In the optimizer we need to pass model parameters(can be accesed using model.parameters()) for the back propagation operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Traning the Model:<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Steps to note\n",
    "    - torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script)\n",
    "    \n",
    "    Few Steps to note:\n",
    "\n",
    "- <b>optimizer.zero_grad()</b>: - will zero out the gradients from previous traning step , in this way gradients won't be   accumulated. This should be done before calculating the gradients at each epoch.\n",
    "- <b>criterion(output, target)</b>: - we feed in the model predicted values along with actual values to calculate the loss.\n",
    "- <b>optimizer.step()</b>: Once we call loss.backward() , gradients will be calculated and we will use this gradients to update the weights in this step using the learning rate defined in optim.SGD(model.parameters(), lr=0.01).\n",
    "- <b>torch.no_grad()</b>: impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop. We generally don't want backpropagation in validation and test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.7252699347989012 Valid Loss:  0.40598030549784503\n",
      "Epoch: 2 Training Loss:  0.36131680904266733 Valid Loss:  0.33169983032469946\n",
      "Epoch: 3 Training Loss:  0.3144735790944348 Valid Loss:  0.29959434183935324\n",
      "Epoch: 4 Training Loss:  0.2850455914158374 Valid Loss:  0.2795327842701226\n",
      "Epoch: 5 Training Loss:  0.2613767802792912 Valid Loss:  0.262356000362585\n",
      "Epoch: 6 Training Loss:  0.2414350723537306 Valid Loss:  0.24396263679179053\n",
      "Epoch: 7 Training Loss:  0.2229606965285105 Valid Loss:  0.22929387559803824\n",
      "Epoch: 8 Training Loss:  0.20704525047913194 Valid Loss:  0.21653890158049763\n",
      "Epoch: 9 Training Loss:  0.1923978082719259 Valid Loss:  0.20423881189587215\n",
      "Epoch: 10 Training Loss:  0.18020653621448826 Valid Loss:  0.19436424857315918\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11): ## run the model for 10 epochs\n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ## evaluation part\n",
    "    with torch.no_grad():\n",
    "        #model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss.append(loss.item())\n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.180090\n",
      "\n",
      "Test Accuracy of     0: 98% (967/980)\n",
      "Test Accuracy of     1: 98% (1117/1135)\n",
      "Test Accuracy of     2: 93% (965/1032)\n",
      "Test Accuracy of     3: 95% (969/1010)\n",
      "Test Accuracy of     4: 93% (921/982)\n",
      "Test Accuracy of     5: 91% (817/892)\n",
      "Test Accuracy of     6: 96% (927/958)\n",
      "Test Accuracy of     7: 93% (964/1028)\n",
      "Test Accuracy of     8: 91% (894/974)\n",
      "Test Accuracy of     9: 93% (946/1009)\n",
      "\n",
      "Test Accuracy (Overall): 94% (9487/10000)\n"
     ]
    }
   ],
   "source": [
    "#Test the train network\n",
    "\n",
    "# initialize lists to monitor test loss and accuracy\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "model.eval() # prep model for evaluation\n",
    "\n",
    "for data, target in test_loader:\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*data.size(0)\n",
    "    #test_loss.append(loss.item())\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    # compare predictions to true label\n",
    "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        label = target.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# calculate and print avg test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            str(i), 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))\n",
    "\n",
    "# Code Credit : Udacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
