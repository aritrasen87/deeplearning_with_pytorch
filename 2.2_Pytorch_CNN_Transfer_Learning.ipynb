{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['flowers_']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Image augmentation and normalization** \n\n- Transforms can be chained together using Compose\n- In image augmentation we randomly flip images, so that our model can detect wrongly oriented images too\n- All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n- We first Resize the image to 256 then crop it to 224, so that it doesnt cut important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = transforms.Compose([\n                                transforms.Resize(256),\n                                transforms.RandomResizedCrop(224),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])\n\ntest_transform = transforms.Compose([\n                                transforms.Resize(256),\n                                transforms.CenterCrop(224),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/flowers_/flowers_/\"\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A call to ImageFolder(Path, Transform) applies our transformations to all the images in the specified directory.\nWe will create a dictorionary called img_dataset for train and test folder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_datasets ={}","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# That's how easily you can for images folders in Pytorch for further operations\nimg_datasets['train']= datasets.ImageFolder(data_dir + '/train', train_transform)\nimg_datasets['test']= datasets.ImageFolder(data_dir + '/test', test_transform)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Classes Present"},{"metadata":{"trusted":true},"cell_type":"code","source":"# these gets extracted from the folder name\nclass_names = img_datasets['train'].classes\nclass_names","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Creating Train & Test DataLoaders"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(img_datasets['train'],\n                                                   batch_size=10,\n                                                   shuffle=True,\n                                                   num_workers=4)\n\ntest_loader = torch.utils.data.DataLoader(img_datasets['test'],\n                                                   batch_size=10,\n                                                   shuffle=True,\n                                                   num_workers=4)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's examing a Batch of training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"images , labels = next(iter(train_loader))\nimages.shape","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"torch.Size([10, 3, 224, 224])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"- 10 - number of images in a single batch\n- 3 - number channels \n- 224 - width & height of the image"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the labels\nlabels","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"tensor([2, 0, 2, 1, 3, 1, 1, 1, 3, 4])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"All of the pretrained models are present inside torchvision , in this tutorial we will use vgg16 pretrained layer.\nPS: In Kaggle to download the pretrained model , you need to set Internet to On in settings."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision.models as models\n\nmodel = models.vgg16(pretrained=True)","execution_count":10,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /tmp/.torch/models/vgg16-397923af.pth\n553433881it [00:07, 73377774.49it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"**Freezing model's layers:**\n\nWe will freeze all the layers in the network except the final layer.\nrequires_grad == False will freeze the parameters so that the gradients are not computed in backward() i.e. weights of these layers won't be trained"},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in model.parameters():\n    param.required_grad = False","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's check the model archietecture\nmodel","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace)\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace)\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"If you remember we have five classes i.e. five class image classification , in the above print out if you look closely the (classifier)\nsection - this is doing something else. We need to change the classifier to make it a 5 class classifier.\n\nwe need to feed the no of input features to the linear layer (classifier[0]) to our newly created linear layer and output would be 5."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_of_inputs = model.classifier[0].in_features\nnum_of_inputs","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"25088"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# restructaring the classifier\nimport torch.nn as nn\nmodel.classifier = nn.Sequential(\n                      nn.Linear(num_of_inputs, 5),\n                        nn.LogSoftmax(dim=1))","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's check the model archietecture again to see the changes \nmodel","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=5, bias=True)\n    (1): LogSoftmax()\n  )\n)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Hope you can see the changes in the classifier layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')\n# move tensors to GPU if CUDA is available\nif train_on_gpu:\n    model.cuda()","execution_count":16,"outputs":[{"output_type":"stream","text":"CUDA is available!  Training on GPU ...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss function and optimizer\ncriterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of epochs to train the model\nn_epochs = 10\n\n\nfor epoch in range(n_epochs):\n    # monitor training loss\n    train_loss = 0.0\n    train_accuracy = 0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train() # prep model for training\n    for data, target in train_loader:\n        if train_on_gpu:\n            data, target = Variable(data.cuda()), Variable(target.cuda())\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss += loss.item()*data.size(0)\n        #calculate accuracy\n        ps = torch.exp(output)\n        top_p, top_class = ps.topk(1, dim=1)\n        equals = top_class == target.view(*top_class.shape)\n        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n    \n# calculate average loss over an epoch\n    train_loss = train_loss/len(train_loader.dataset)\n\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n            epoch+1, \n            train_loss\n            ))\n    print(f\"Train accuracy: {train_accuracy/len(train_loader):.3f}\")\n","execution_count":18,"outputs":[{"output_type":"stream","text":"Epoch: 1 \tTraining Loss: 0.972416\nTrain accuracy: 0.770\nEpoch: 2 \tTraining Loss: 0.912946\nTrain accuracy: 0.829\nEpoch: 3 \tTraining Loss: 0.954519\nTrain accuracy: 0.843\nEpoch: 4 \tTraining Loss: 0.971870\nTrain accuracy: 0.847\nEpoch: 5 \tTraining Loss: 0.906120\nTrain accuracy: 0.853\nEpoch: 6 \tTraining Loss: 0.869895\nTrain accuracy: 0.876\nEpoch: 7 \tTraining Loss: 0.840232\nTrain accuracy: 0.881\nEpoch: 8 \tTraining Loss: 0.816031\nTrain accuracy: 0.876\nEpoch: 9 \tTraining Loss: 0.773499\nTrain accuracy: 0.894\nEpoch: 10 \tTraining Loss: 0.819791\nTrain accuracy: 0.895\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Test Performence\ntest_accuracy = 0\nmodel.eval() # prep model for evaluation\nfor data, target in test_loader:\n    if train_on_gpu:\n        data, target = Variable(data.cuda()), Variable(target.cuda())\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    # calculate the loss\n    loss = criterion(output, target)\n    #calculate accuracy\n    ps = torch.exp(output)\n    top_p, top_class = ps.topk(1, dim=1)\n    equals = top_class == target.view(*top_class.shape)\n    test_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n\nprint(f\"Test accuracy: {test_accuracy/len(test_loader):.3f}\")","execution_count":19,"outputs":[{"output_type":"stream","text":"Test accuracy: 0.840\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Accuracy can be improved by changing the classifer archietecture !! "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}