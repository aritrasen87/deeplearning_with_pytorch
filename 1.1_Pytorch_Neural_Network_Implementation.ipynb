{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section with one sample example we saw how to train a neaural network from scratch. In this notebook we'll use nn.Module and nn.Parameter along with autograd, optim utility packages provided within PyTorch. We subclass nn.Module (which itself is a class and able to keep track of state). nn.Module has a number of attributes and methods (such as .parameters() and .zero_grad()) which we will be using to perform different operations required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial ,  we will create a Deep Learning model for building a handwritten digit classifier. We will make use of the MNIST dataset included in the torchvision package.\n",
    " \n",
    "<b>Data Preprocessing</b>:\n",
    "\n",
    " Mandatory first step is to do the basic data pre-processing steps , using the a utility called <b>transforms</b> which comes from torchvision package.\n",
    " \n",
    " We will do two below mentioned basic data preprocessing operations (this will be explained more detail in case of CNN tutorial).\n",
    " \n",
    "- Transform the raw dataset into tensors.\n",
    "- Normalize the dataset.\n",
    "\n",
    "We will also import the dataset from torch vision package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training\n",
    "train_data = MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of train data\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader helps us to create batches of data , we will go deeper into DataLoader in next tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data loader preparation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of the input/target data\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that input data is not flattened to pass through the linear layers. \n",
    "\n",
    "So we will need to reshape the batch in a format of (batch size , no of features).Shape of target data is as expected.\n",
    "\n",
    "number of features -> 1 * 28 * 28 = 784 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "We create a new class (which inherits the properties from the base class from nn package called Module) to define the archietecture of the Neaural Network. \n",
    "\n",
    "- Layer defination should be inside the constructor of the class.\n",
    "- Forward propagation step should be included inside forward method.\n",
    "\n",
    "Activations(Relu,Sigmoid,Tanh etc) and loss functions(cross entropy,softmax etc) comes from torch.nn.functional. This module contains all the functions in the torch.nn module.\n",
    "\n",
    "Syntax of nn.Linear() is (input size, output size)\n",
    "\n",
    "This NN architecture below represents the 784 nodes (28*28 pixels) in the input layer, 256 in the hidden layer, and 10 in the output layer(0-9 numbers). Inside the forward function, we will use the relu activation function in the hidden layer which present under torch.nn.functional module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input tensor is flattened \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the loss and optimizer functions\n",
    "\n",
    "- <b>Loss Function</b> : Here we have used the CrossEntropyLoss() function.Generally loss assigned to `criterion`. for MNIST classification , we generally use softmax function to predict class probabilities. With a softmax output, you want to use cross-entropy as the loss.\n",
    "\n",
    "Few things to keep in mind while using the CrossEntropyLoss() in Pytorch \n",
    "\n",
    "    - CrossEntropyLoss criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "    - The input is expected to contain scores for each class.\n",
    "\n",
    "This means we need to pass in the raw output of our network into the loss, not the output of the softmax function.That's why there is no activation after self.fc2(x).\n",
    "\n",
    "- <b>Optimizer</b> : Pytorch also has a package with various optimization algorithms, torch.optim. We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.Below are the few availavle optimizer in pytorch -\n",
    "\n",
    "    - optim.Adam\n",
    "    - optim.RMSprop\n",
    "    - optim.SGD\n",
    "    - optim.Adagrad\n",
    "    \n",
    "In the optimizer we need to pass model parameters(can be accesed using model.parameters()) for the back propagation operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Traning the Model:<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.7252699347989012 Valid Loss:  0.40598030549784503\n",
      "Epoch: 2 Training Loss:  0.36131680904266733 Valid Loss:  0.33169983032469946\n",
      "Epoch: 3 Training Loss:  0.3144735790944348 Valid Loss:  0.29959434183935324\n",
      "Epoch: 4 Training Loss:  0.2850455914158374 Valid Loss:  0.2795327842701226\n",
      "Epoch: 5 Training Loss:  0.2613767802792912 Valid Loss:  0.262356000362585\n",
      "Epoch: 6 Training Loss:  0.2414350723537306 Valid Loss:  0.24396263679179053\n",
      "Epoch: 7 Training Loss:  0.2229606965285105 Valid Loss:  0.22929387559803824\n",
      "Epoch: 8 Training Loss:  0.20704525047913194 Valid Loss:  0.21653890158049763\n",
      "Epoch: 9 Training Loss:  0.1923978082719259 Valid Loss:  0.20423881189587215\n",
      "Epoch: 10 Training Loss:  0.18020653621448826 Valid Loss:  0.19436424857315918\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11): ## run the model for 10 epochs\n",
    "    train_loss = []\n",
    "    \n",
    "    ## training part \n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few Steps to note:\n",
    "\n",
    "- <b>optimizer.zero_grad()</b>: - will zero out the gradients from previous traning step , in this way gradients won't be   accumulated. This should be done before calculating the gradients at each epoch.\n",
    "- <b>criterion(output, target)</b>: - we feed in the model predicted values along with actual values to calculate the loss.\n",
    "- <b>optimizer.step()</b>: Once we call loss.backward() , gradients will be calculated and we will use this gradients to update the weights in this step using the learning rate defined in optim.SGD(model.parameters(), lr=0.01)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
