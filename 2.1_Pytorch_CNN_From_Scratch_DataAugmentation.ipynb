{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will Train a ConvNet to classify images from Cifar10 databaset.\n",
    "\n",
    "The Cifar10 dataset is included in torch-vision package which we installed in our first tutorial.\n",
    "\n",
    "In this image classification tutorial we classify imgage from 10 classes given below - \n",
    "\n",
    "plane, car, bird, cat, deer, dog ,  frog, horse, ship, truck\n",
    "\n",
    "By specifying <b>train = True</b> we will load the training data from the dataset and test dataset with <b>train = False</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 2\n",
    "# how many samples per batch to load\n",
    "batch_size =  10 \n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop(size=224),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size\n",
    "                                           , num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transforms from torchvision Packages gives us immense no of image transformation / augmentation capability , some of the examples given in below sample code -\n",
    "\n",
    "```python \n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms_train = transforms.Compose([transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(), # randomly flip\n",
    "    transforms.RandomRotation(degrees=15),  # Random rotation\n",
    "    transforms.CenterCrop(size=224), # Center image crop\n",
    "    transforms.ColorJitter(),                                            \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)])\n",
    "\n",
    "data_transforms_test = transforms.Compose([transforms.Resize(size=256),\n",
    "    transforms.CenterCrop(size=224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std)])\n",
    "```\n",
    "\n",
    "Documentation Link - https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "##### Please note it is advisable that you use different transformation operation in train & test set (as shown above)\n",
    "- for the simplicity I have used a single transofrmation for tranin & test set in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the datasets\n",
    "\n",
    "#### Get a batch from the training data\n",
    "The DataLoader object divides the dataset into batches. Here we examine the first batch of the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images_batch, labels_batch = iter(train_loader).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check out the shape of this batch of images\n",
    "The first dimension gives the number of images. The next dimension represents the number of channels. The last two give the image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the archietecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.25)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convolutional layer (sees 32x32x3 image tensor)\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        # convolutional layer (sees 16x16x16 tensor) -> calulation shown below \n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        # convolutional layer (sees 8x8x32 tensor)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # linear layer (64 * 4 * 4 -> 500)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 500)\n",
    "        # linear layer (500 -> 10)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        # dropout layer (p=0.25)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output volume can be calculated with below formula:\n",
    "\n",
<<<<<<< HEAD:2.1_Pytorch_CNN_From_Scratch_DataAugmentation.ipynb
    "- Input: n X n X nc \n",
    "- Filter: f X f X nc\n",
    "- Padding: p\n",
    "- Stride: s\n",
    "- Output: [((n+2p-f)/s)+1] X [((n+2p-f)/s)+1] X nc’   (height X width X no of output channels)\n",
=======
    "- Input: n X n X nc\n",
    "- Filter: f X f X nc\n",
    "- Padding: p\n",
    "- Stride: s\n",
    "- Output: [(n+2p-f)/s+1] X [(n+2p-f)/s+1] X nc’   (height * width * no of channels)\n",
>>>>>>> 83abefba47154b3e78e64e47f589e98535414776:2.1_Pytorch_CNN_From_Scratch.ipynb
    "\n",
    "nc is the number of channels in the input and filter, while nc’ is the number of filters.\n",
    "\n",
    "From the above structure you can see that height/width is getting reduced and number of channels are getting incresed.\n",
    "\n",
    "Example calulating the output of first convolution + pooling layer operation - \n",
    "\n",
    "Input image shape - 32(n) X 32(n) X 3(nc)\n",
    "\n",
    "#### 1. ConVNet filter operation - self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "\n",
    "Filter shape - 3 (f) X 3 (f) X 3(nc) \n",
    "Padding : P = 1\n",
    "Stride : s = 1 (default value)\n",
    "output channels - 16 (kernel_size)\n",
    "\n",
    "putting it in the formula given above - \n",
    "\n",
    "[((n+2p-f)/s)+1] X [((n+2p-f)/s)+1] X nc’ \n",
    "\n",
    "[((32 + 2X1 - 3) / 1) + 1)] X [((32 + 2X1 - 3) / 1)) + 1)] X 16\n",
    "\n",
    "output shape -> 32 X 32 X 16\n",
    "\n",
    "##### 2. output of conv1 is passed through max pooling layer.\n",
    "\n",
    "self.pool = nn.MaxPool2d(2, 2) -> filter of 2 X 2.\n",
    "\n",
    "this will shrink the height & width by half , however no of channels will remain same.\n",
    "\n",
    "input to the pooling layer - 32 X 32 X 16\n",
    "\n",
    "output of the pooing layer - 32/2 X 32/2 X 16 -> 16 X 16 X 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the learning rate , loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2000/5000], Loss: 1.1039\n",
      "Epoch [1/5], Step [4000/5000], Loss: 1.0615\n",
      "Epoch [2/5], Step [2000/5000], Loss: 0.7608\n",
      "Epoch [2/5], Step [4000/5000], Loss: 1.0013\n",
      "Epoch [3/5], Step [2000/5000], Loss: 0.6833\n",
      "Epoch [3/5], Step [4000/5000], Loss: 0.7023\n",
      "Epoch [4/5], Step [2000/5000], Loss: 0.5616\n",
      "Epoch [4/5], Step [4000/5000], Loss: 0.5238\n",
      "Epoch [5/5], Step [2000/5000], Loss: 0.7064\n",
      "Epoch [5/5], Step [4000/5000], Loss: 0.7451\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 2000 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 71.86%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {}%'\\\n",
    "          .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model archietecture credit: Udacity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
