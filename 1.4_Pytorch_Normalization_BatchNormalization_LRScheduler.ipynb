{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this tutorial ,  we will create a Deep Learning model for building a handwritten digit classifier. We will make use of the MNIST dataset included in the torchvision package.\n",
    " \n",
    " Mandatory first step is to do the basic data pre-processing steps , using the a utility called transforms which comes from \n",
    " torchvision package we will do two below mentioned basic data preprocessing operations (this will be explained more detail in case of CNN).\n",
    " \n",
    "- Transform the raw dataset into tensors.\n",
    "- Normalize the dataset.\n",
    "\n",
    "We will also import the dataset from torch vision package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity we are only using the training dataset in this tutorial\n",
    "\n",
    "Test and Validation dataset have been used in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the training dataset\n",
    "train_data = MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of train dataset\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# data loader preparation\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader creates iterables for all the batches and we will use this inside the traning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of the input/target data\n",
    "for data, target in train_loader:\n",
    "    print(data.shape)\n",
    "    print(target.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization:\n",
    "\n",
    "Added Batch Normalization after the linear but before the non linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=512) # batch norm layer 1\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=256) # batch norm layer 2\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=128)  # batch norm layer 3                         \n",
    "        self.fc4 = nn.Linear(128, 56)\n",
    "        self.bn4 = nn.BatchNorm1d(num_features=56)   # batch norm layer 4 \n",
    "        self.fc5 = nn.Linear(56, 10)\n",
    "        \n",
    "        #drop out with 0.3 probability\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input tensor is flattened \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        # applied dropout layer\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.relu(self.bn3(self.fc3(x))))\n",
    "        x = self.dropout(F.relu(self.bn4(self.fc4(x))))\n",
    "        \n",
    "        #no dropout at the output layer\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler:\n",
    "\n",
    "Here we will use the StepLR i.e. Step Learning Rate scheduler. Below is the reference and example from Pytorch doc:\n",
    "\n",
    "```python\n",
    "- torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)\n",
    "```\n",
    "\n",
    "Sets the learning rate of each parameter group to the initial lr decayed by gamma every step_size epochs. \n",
    "\n",
    "Parameters:\t\n",
    "#### optimizer (Optimizer) – Wrapped optimizer.\n",
    "#### step_size (int) – Period of learning rate decay.\n",
    "#### gamma (float) – Multiplicative factor of learning rate decay. Default: 0.1.\n",
    "#### last_epoch (int) – The index of last epoch. Default: -1.\n",
    "\n",
    "Example:\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "#### Assuming optimizer uses lr = 0.05 for all groups\n",
    "#### lr = 0.05     if epoch < 5\n",
    "#### lr = 0.005    if 5 <= epoch < 10\n",
    "#### lr = 0.0005   if 10 <= epoch < 15\n",
    "\n",
    "Examples describes that the initial learning rate defined in the <b>optimizer</b> step was 0.05 , which will be reducuded after every 5 epochs.Learning Rate will be decreased with the multiplication factor of 0.1(i.e. the value defined in gamma).\n",
    "\n",
    "Other learning rate scheduler details are here - https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LR scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Creating a device object \n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the model to avialable 'device'\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Traning the Model:</b>\n",
    "    \n",
    "Note: scheduler.step() has to be added to decay the learning rate with epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.591511638003091\n",
      "Epoch: 2 Training Loss:  0.27450931679923085\n",
      "Epoch: 3 Training Loss:  0.21339781960472465\n",
      "Epoch: 4 Training Loss:  0.18062251378704483\n",
      "Epoch: 5 Training Loss:  0.1567279348600035\n",
      "Epoch: 6 Training Loss:  0.13593307446455583\n",
      "Epoch: 7 Training Loss:  0.1244783455634024\n",
      "Epoch: 8 Training Loss:  0.11661969905040072\n",
      "Epoch: 9 Training Loss:  0.1055293292296119\n",
      "Epoch: 10 Training Loss:  0.09655629589455202\n",
      "Epoch: 11 Training Loss:  0.0824143057805486\n",
      "Epoch: 12 Training Loss:  0.07365655489324126\n",
      "Epoch: 13 Training Loss:  0.06981207851921985\n",
      "Epoch: 14 Training Loss:  0.06699953551093737\n",
      "Epoch: 15 Training Loss:  0.0663414308197874\n",
      "Wall time: 10min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, 16): ## run the model for 15 epochs\n",
    "    train_loss = []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    scheduler.step() # for LR scheduler\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # Move input and label tensors to the avialable device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #Reshaping the input data before sending into the model\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ## 1. forward propagation\n",
    "        output = model(data)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
